\subsection{Gas de Fermi}{\label{sec:intro_fermi_gas}}

En los sistemas cuánticos, la indistingubilidad de las particulas juega un rol esencial.
Esta indistingubilidad obliga a todos los autoestados $\ket{\psi}$ del sistema a ser autoestados del operador permutación de 2 partículas $\hat{P}$, cuyos únicos autovalores son $\pm 1$.
De esta manera, se asegura la invarianza de los observables ante $\hat{P}$
\[\bra{\psi'}\hat{A}\ket{\psi'} = \bra{\psi}\hat{P}^\dagger\hat{A}\hat{P}\ket{\psi} = (\bra{\psi}\pm)\hat{A}(\pm\ket{\psi}) = \bra{\psi}\hat{A}\ket{\psi}\]

Los fermiones son aquellas particulas cuyas funciones de onda tienen autovalor negativo ante cualquier permutación de 2 partículas ($\hat{A}\ket{\psi} = -\ket{\psi}$) y se dice que son antisimétricas.
De esta propiedad surge el principio de exclusión de Pauli, que prohibe la ocupación de un mismo estado por más de una partícula.
De otra forma, sería imposible lograr que la función de onda sea antisimétrica, dado que si tuviesemos dos partículas $i$ y $j$ en el mismo estado, la permutación $\hat{P}_{ij}$ mantendría
$\ket{\psi}$ invariante en lugar de cambiar su signo.

En general, los fermiones poseen spin semi-entero $s=l+1/2$, razón por la que los nucleones (protones y neutrones) y los electrones son fermiones.
La exclusión de Pauli tiene efectos importantes en la termodinámica de fermiones a bajas temperaturas.
El sistema de fermiones no interactuantes es el más simple, cuya estadística es bien conocida, siendo su función de partición gran canónica y ocupación media\textbf{CITA PATHRIA}
\begin{equation}
 \frac{PV}{k_BT} \equiv \log Z = \sum_\varepsilon \log(1+ze^{-\beta\varepsilon})
\end{equation}
\begin{equation}
 N = \sum_\varepsilon \langle n_\varepsilon\rangle = \sum_\varepsilon \frac{1}{z^{-1}e^{\beta\varepsilon}+1}
\end{equation}
con $z=e^{\beta\mu}$ la fugacidad, $\beta=1/k_BT$ y $\mu$ el potencial químico del sistema.
Los niveles de energía de una partícula $\varepsilon=p^2/2m$ están discretizados y dependen del volumen $V$, con $\Delta\varepsilon\sim V^{-1/3}$.

En el límite termodinámico ($V\to\infty$), la diferencia entre energías $\Delta\varepsilon\to 0$ y podemos transformar la sumatoria en una integral en el espacio de fases.
Realizando las integrales triviales, resulta
\begin{equation}
 \frac{PV}{k_BT} \equiv \log Z = \frac{4\pi V}{h^3} \int_0^\infty \log(1+ze^{-\beta p^2/2m})p^2dp = \frac{gV}{\lambda^3}f_{5/2}(z)
\end{equation}
\begin{equation}{\label{eq:N_cont}}
 N = \frac{4\pi V}{h^3} \int_0^\infty \frac{1}{z^{-1}e^{\beta p^2/2m}+1} p^2dp = \frac{gV}{\lambda^3}f_{3/2}(z)
\end{equation}
donde $g$ es un factor de degeneración asociado a grados de libertad internos (spin, por ejemplo) y  $\lambda$ es la longitud de onda térmica
\[ \lambda = \sqrt{ \frac{2\pi\hbar^2}{mk_BT} } \]
y las $f_\nu(z)$ son las funciones de Fermi-Dirac definidas según
\[ f_\nu(z) = \frac{1}{\Gamma(\nu)}\int_0^\infty \frac{x^{\nu-1}}{z^{-1}e^x+1} = \sum_{l=1}^\infty (-1)^{l+1}\frac{z^l}{l^\nu}\]

Para un sistema NVT, debemos obtener la fugacidad $z$ invirtiendo \eqref{eq:N_cont}, lo que introduce el problema de computar las funciones de Fermi-Dirac $f_\nu(z)$.
Al ser series alternadas, la convergencia de las sumas parciales es muy lenta y debemos recurrir a otros métodos, que detallamos en el Apéndice \textbf{APENDICE DIRAC}.
Independientemente de esto, puede verse que obtendremos un $\mu(N/V,T) = \mu(\rho, T)$, razonable dado que es una magnitud intensiva del sistema.

El integrando de \eqref{eq:N_cont} puede considerarse como la densidad de partículas $f(p)$ con impulso de módulo $p$.
Esta distribución de impulsos es característica de un gas de fermiones isótropo y homogéneo.
Equivalentemente, podriamos analizar la distrubición de energías cinéticas $\varepsilon=p^2/2m$ mediante un cambio de variables sobre la integral de \eqref{eq:N_cont}
($d\varepsilon = pdp/m = \sqrt{2m\varepsilon}dp/m$)

\[ N = \frac{4\pi V}{h^3} \int_0^\infty \frac{1}{e^{\beta(\varepsilon-\mu)}+1} 2m\varepsilon\frac{m}{\sqrt{2m\varepsilon}}d\varepsilon =
\int_0^\infty \frac{4\pi V\sqrt{2m^3\varepsilon}}{h^3}\frac{1}{e^{\beta(\varepsilon-\mu)}+1} d\varepsilon\]

Por lo tanto, la distribución de impulsos toma la forma
\begin{equation}{\label{eq:dist_FD}}
 f_{FD}(\varepsilon) = \frac{4\pi V \sqrt{2m^3\varepsilon}}{h^3}\frac{1}{e^{\beta(\varepsilon-\mu)}+1} = \frac{G(\varepsilon)}{e^{\beta(\varepsilon-\mu)}+1}
 %= N\frac{V}{\lambda^3}\sqrt{\frac{4\beta^3\varepsilon}{\pi}}\frac{1}{e^{\beta(\varepsilon-\mu)}+1}
\end{equation}
donde $G(\varepsilon)$ es la degeneración de estados con energía $\varepsilon$ y asumimos $\mu=\mu(N,V,T)$.

En el límite de altas temperaturas (o baja densidad), esperamos que el sistema pierda sus propiedades cuánticas y recuperemos el gas clásico, donde las partículas
se vuelven distinguibles nuevamente.
La noción de alta temperatura (o baja densidad) viene dada por el valor del producto $\lambda^3\rho \sim T^{-3/2}\rho$, donde vemos que $T\to\infty$ resulta equivalente a $\rho\to0$
y viceversa para baja temperatura (o alta densidad), donde las propiedades cuánticas son más apreciables.

Para un sistema de partículas clásico interactuando con potenciales dependientes de la posición $U(\mathbf{q}_1,..,\mathbf{q}_N)$, la distribución de impulsos
corresponde a la de Boltzmann (ver \textbf{Apéndice \ref{ap:boltzmann}}).
\begin{equation}{\label{eq:dist_MB}}
 f_{MB}(\varepsilon) = N\sqrt{\frac{4\beta^3\varepsilon}{\pi}}e^{-\beta\varepsilon}
\end{equation}


\subsection{Potencial de Pauli}{\label{sec:intro_pauli}}

Dado que la distribución \eqref{eq:dist_MB} es común a todo sistema de partículas distinguibles (o clásicas) cuyos Hamiltonianos cuya dependencia con el momento
sea a través de la energía cinética (ver  \textbf{Apéndice \ref{ap:boltzmann}})
\[ H(\mathbf{q}_1,..,\mathbf{q}_N,\mathbf{p}_1,..,\mathbf{p}_N) = \sum_i \frac{p_i^2}{2m} + U(\mathbf{q}_1,..,\mathbf{q}_N)\]
la única forma de obtener una distribución de energías distinta con partículas distinguibles es introduciendo potenciales dependientes de momentos al Hamiltoniano.

Esto puede hacerse de infinitas maneras, pero en particular buscamos una cuya distribución resulte similar a \eqref{eq:dist_FD}.
Para esto, necesitamos poder expresar la noción de Exclusión de Pauli con un potencial de interacción de dos partículas $V(\mathbf{q}_1,..,\mathbf{q}_N;\mathbf{p}_1,..,\mathbf{p}_N)$.
Esto lo haremos imponiendo un ``costo energético'' cada vez que 2 partículas tengan un $\mathbf{q}, \mathbf{p}$ similar.
Buscamos lograr esto agregando al Hamiltoneano un potencial de interacción de 2 partículas que llamaremos \textit{potencial de Pauli} definido según

\begin{equation}{\label{eq:def_int_pauli}}
 V_P(\mathbf{q}_1,\mathbf{q}_2;\mathbf{p}_1,\mathbf{p}_2) = De^{-\frac{1}{2}\left( \frac{|\mathbf{q}_1-\mathbf{q}_2|^2}{q_o^2} +\frac{|\mathbf{p}_1-\mathbf{p}_2|^2}{p_o^2} \right)}
\end{equation}
donde surge la noción de distancia en el espacio de fases $s^2 = \frac{|\mathbf{q}_1-\mathbf{q}_2|^2}{q_o^2} +\frac{|\mathbf{p}_1-\mathbf{p}_2|^2}{p_o^2}$.

Podemos visualizar los parámetros $q_o$ y $p_o$ como los ejes de un elipsoide en $\mathbb{R}^6$ (el espacio de fases) donde $s^2 \leq 1$ y el costo energético de cualquier partícula
que ingrese es $\approx D$.
A energías (o temperaturas) bajas, este costo energético puede ser suficientemente alto como para evitar la superposición de estos elipsoides en el espacio de fases, generando efectivamente
la noción de exclusión de Pauli: dos partículas no pueden estar en el mismo estado $(\mathbf{q}, \mathbf{p})$.
Es importante notar que este potencial no es de núcleo duro y, por lo tanto, es posible que dos partículas se superpongan para energías suficientemente altas donde $D$ sea despreciable.
Esto es deseable dado que para energías (o temperaturas) altas esperamos recuperar el gas clásico de Maxwell-Boltzmann al desaparecer $V_P$.

Este potencial fue inicialmente propuesto por Wiletz \textbf{CITAS WILETZ} y continuado por Dorso y Randrup \textbf{CITAS DORSO PAULI} y utilizado en múltiples trabajos desde entonces.
Los parámetros $D$, $q_o$ y $p_o$ varian en cada trabajo, adaptándose según la característica que se busque analizar.
Existen formulaciones alternativas como la de Piekarewicz \textbf{CITA PIEKA} que proponen distinto tipo de exclusión; exclusión en $q$ y en $p$ independientemente.

La dependencia del potencial con el impulso trae aparejada una serie de dificultades a la hora de hacer Dinámica Molecular.
Más allá de la inmediata dificultad para evolucionar el sistema (que analizaremos en la sección que sigue), surge la dificultad de definir algunas magnitudes termodinámicas del
sistema como la presión $P$ y la temperatura $T$.
El teorema del virial (ver Apéndice \ref{ap:teo_virial}) nos permite calcular ambas magnitudes según
\begin{equation}{\label{eq:virial_T}}
k_B T =  \frac{1}{3N}\left<\sum_{i=1}^{3N} \dot{q}_ip_i \right>
=  \frac{1}{3N}\left<\sum_{i=1}^{3N} \frac{p_i^2}{m} \right> - \frac{1}{3V}\left< \sum_{i=1}^{3N}\sum_{j\neq i} \dpart{V_{ij}^{(P)}}{p_i}p_i \right>
\end{equation}

\begin{equation}{\label{eq:virial_P}}
P = \frac{1}{3V}\left< \sum_{i=1}^{3N} \dot{q}_ip_i + \dot{p}_iq_i \right>
= \frac{1}{3V}\left< \sum_{i=1}^{3N} \frac{p_i^2}{m} \right> - \frac{1}{3V}\left< \sum_{i=1}^{3N}\sum_{j\neq i} \dpart{V_{ij}^{(P)}}{q_i}q_i+\dpart{V_{ij}^{(P)}}{p_i}p_i \right>
\end{equation}

\subsection{Integración simpléctica de sistemas hamiltonianos}

La evolución de las $d$ coordenadas generalizadas $q \equiv (q_1,..,q_d)\in\mathbb{R}^{d}$ y $p \equiv (p_1,..,p_d)\in\mathbb{R}^{d}$ está definida por el Hamiltoneano
$H(q_1,..,q_d,p_1,..,p_d) \equiv H(q,p)$  a través de las ecuaciones de Hamilton
\begin{align*}
 \dot{q} &= \dpart{H}{p}(q,p) \\
 \dot{p} &= -\dpart{H}{q}(q,p)
\end{align*}
que podemos compactar definiendo $y=(p,q)\in\mathbb{R}^{2d}$ y la matriz $J = \begin{pmatrix}0 & \mathbb{I} \\-\mathbb{I} & 0\end{pmatrix}$ (con la propiedad $J^{-1} = J^T = -J$)
\begin{equation}{\label{eq:ec_hamilton}}
 \dot{y} = J^{-1}\nabla H(y)
\end{equation}

Para un Hamiltoneano que no depende explícitamente del tiempo, la ecuación de evolución \ref{eq:ec_hamilton} asegura la conservación de $H(q,p)$ en el tiempo
\[ \frac{dH}{dt}(y) = \nabla H(y)\cdot \dot{y} = \nabla H(y)^T J^{-1}\nabla H(y) = 0\]
donde usamos que $(a,b)J^{-1}\begin{pmatrix}a\\b\end{pmatrix} = (b, -a)\begin{pmatrix}a\\b\end{pmatrix} = 0$ $\forall a,b$.
Esto no es más que la conocida conservación de la energía.

\subsubsection{Transformaciones simplécticas}{\label{sec:trans_simp}}

Sin embargo, los sistemas hamiltoneanos tienen propiedades adicionales asociadas a la conservación del área en el espacio de fases de $y$.
Supongamos dos vectores
\[ \mu = \begin{pmatrix} \mu^p\\ \mu^q \end{pmatrix} \qquad \eta = \begin{pmatrix} \eta^p\\ \eta^q \end{pmatrix}  \]
con $\mu^p, \mu^q, \eta^p, \eta^q\in\mathbf{R}^{d}$ las ``componentes $p$ y $q$'' de estos vectores.
Definimos el mapeo bilinear $\omega(\mu,\eta)$ como la suma de las areas orientadas de cada par de componentes $p_i,q_i$
\begin{equation}
 \omega(\mu, \eta) = \sum_{i=1}^{3N} \det \begin{pmatrix} \mu^p_i & \eta^p_i \\ \mu^q_i & \eta^q_i \end{pmatrix} = \sum_{i=1}^{3N} \mu^p_i\eta^q_i - \mu^q_i\eta^p_i = \mu^T J \eta
\end{equation}

Con esta noción de área, decimos que un mapeo diferenciable $g:U\subseteq\mathbb{R}^{2d}\to\mathbb{R}^{2d}$ es \textit{simpléctico} si su matriz jacobiana $g'(p,q)$ cumple alguna de las condiciones
equivalentes \[ g'(p,q)^T J g'(p,q) = J \qquad \text{ o } \qquad \omega(g'(p,q)\mu, g'(p,q)\eta) = \omega(\mu, \eta) \]

Para visualizar esto, supongamos $M$ una subvariedad de dimensión 2 de $U$ parametrizada por alguna función suave $\psi(s,t)$ tal que $M=\psi(K)$ para $K\subseteq\mathbb{R}^2$.
Podemos considerar a $M$ como la unión de infinitos paralelogramos infinitesimales definidos por los vectores
\[ \dpart{\psi}{s}(s,t)ds \quad \text{ y } \quad \dpart{\psi}{t}(s,t)dt \]

Sumando sobre todos estos paralogramos, obtenemos el área de $M$ como
\begin{equation}{\label{eq:area_orien_simp}}
 \Omega(M) = \iint_K \omega\left(\dpart{\psi}{t}(s,t), \dpart{\psi}{s}(s,t)\right) ds dt
\end{equation}
y resulta inmediato ver que este volumen es invariante ante una transformación simpléctica $g$.

\begin{theorem}{\label{teo:preservacion_vol}}
 Sea un mapeo diferenciable $g:U\subseteq\mathbb{R}^{2d}\to\mathbb{R}^{2d}$ simpléctico. Luego, preserva $\Omega(M)$
 \[ \Omega(g(M)) = \Omega(M) \]
 para toda variedad $M\subseteq\mathbb{R}^2$ que pueda representarse como la imagen de una función diferenciable $\psi$.
\end{theorem}
\begin{proof}
\begin{align*}
 \Omega(g(M)) &=  \iint_K \omega\left(\dpart{(g\circ\psi)}{t}(s,t), \dpart{(g\circ\psi)}{s}(s,t)\right) ds dt \\
&= \iint_K \omega\left(g'(\psi(s,t))\dpart{\psi}{t}(s,t), g'(\psi(s,t))\dpart{\psi}{s}(s,t)\right) ds dt = \iint_K \omega\left(\dpart{\psi}{t}(s,t), \dpart{\psi}{s}(s,t)\right) ds dt = \Omega(M)
\end{align*}
donde utilizamos la propiedad de $g$ simpléctica.
\end{proof}

Finalmente, nos basta ver que el mapeo de evolución temporal $\varphi_t(p_o,q_o) = (p(t;p_o,q_o), q(t;p_o,q_o))$ donde $p(t;p_o,q_o), q(t;p_o,q_o)$ son las soluciones de \eqref{eq:ec_hamilton}
con condiciones iniciales $p(0)=p_o$ y $q(0)=q_o$.
Reemplazando $y$ por $\varphi_t(p_o,q_o)$ en \eqref{eq:ec_hamilton} y derivando respecto de $y_o=(p_o,q_o)$ tenemos
\begin{align*}
 \dpart{\dot{\varphi_t}}{y_o} &= J^{-1}\dpart{}{y_o} \nabla H(\varphi_t) \\
 \frac{d}{dt}\left(\dpart{\varphi_t}{y_o}\right) &= J^{-1} \nabla^2 H(\varphi_t)\dpart{\varphi_t}{y_o}
\end{align*}

donde $\nabla^2 H$ es la matriz hessiana de $H$ y, cabe destacar, es simétrica. Esta propiedad es esencial, pues de ella se deriva el siguiente teorema

\begin{theorem}{\label{teo:preservacion_evol}}
  Sea $H(p,q)$ una función doblemente diferenciable en $U\subseteq\mathbb{R}^{2d}$. Luego, para cada $t$ fijo, el mapeo $\varphi_t$ es una transformación simpléctica
  para cualquier condición inicial $y_o$.
\end{theorem}
\begin{proof}
 Para este caso, tenemos $g'(p,q) = \dpart{\varphi_t}{y_o}(y_o)$ y, por lo tanto,
 \begin{align*}
  \frac{d}{dt}\left[ \left(\dpart{\varphi_t}{y_o}\right)^T J \left(\dpart{\varphi_t}{y_o}\right) \right]
  &= \left[\frac{d}{dt}\left(\dpart{\varphi_t}{y_o}\right)^T\right] J \left(\dpart{\varphi_t}{y_o}\right) + \left(\dpart{\varphi_t}{y_o}\right)^T J \left[\frac{d}{dt}\left(\dpart{\varphi_t}{y_o}\right)\right] \\
  &= \left[J^{-1} \nabla^2 H(\varphi_t)\dpart{\varphi_t}{y_o}\right]^T J \left(\dpart{\varphi_t}{y_o}\right) + \left(\dpart{\varphi_t}{y_o}\right)^T J \left[J^{-1} \nabla^2 H(\varphi_t)\dpart{\varphi_t}{y_o}\right] \\
  &= \left(\dpart{\varphi_t}{y_o}\right)\nabla^2 H(\varphi_t)\left(J^{-1} \right)^T J \left(\dpart{\varphi_t}{y_o}\right) + \left(\dpart{\varphi_t}{y_o}\right)^T \nabla^2 H(\varphi_t)\dpart{\varphi_t}{y_o} = 0
 \end{align*}
 donde usamos que $J^{-1} = J^T$ y $J^2 = -\mathbb{I}$.

 Por lo tanto, $\left(\dpart{\varphi_t}{y_o}\right)^T J \left(\dpart{\varphi_t}{y_o}\right)$ es constante $\forall t$ y para $t=0$ tenemos $\varphi_0(y_o) = y_o$ por lo que $\dpart{\varphi_0}{y_o}=\mathbb{I}$
 \[ \left(\dpart{\varphi_t}{y_o}\right)^T J \left(\dpart{\varphi_t}{y_o}\right) = \left(\dpart{\varphi_o}{y_o}\right)^T J \left(\dpart{\varphi_o}{y_o}\right) = \mathbb{I}^T J \mathbb{I} = J\]
\end{proof}


\subsubsection{Integradores simplécticos}{\label{sec:int_simpl}}

Dado que la simplecticidad es una propiedad fundamental de todo sistema hamiltoneano, resulta razonable plantear que el integrador que utilicemos para evolucionar temporalmente
el sistema también lo sea.
Consideramos un integrador de un paso es simpléctico si el mapeo asociado $y_1=\Phi_h(y_o)$ es simpléctico.

El primer integrador simpléctico de interés es \textit{MidPoint Rule} (MPR), cuyo esquema es
\begin{equation}{\label{eq:MPR}}
 y_{n+1} = y_n + hJ^{-1}\nabla H\left(\frac{y_{n+1}+y_n}{2}\right)
\end{equation}

Veamos que es de orden 2 tomando $y(t_o+h)$ como la solución exacta a \eqref{eq:ec_hamilton} con $y(t_o)=y_o$
\[ y(t_o+h) = y_o + hJ^{-1}\nabla H(y_o) + O(h^2) \]
\[ y_1 = y_o + hJ^{-1}\nabla H\left(\frac{y_1+y_o}{2}\right) =  y_o + hJ^{-1}\nabla H\left(y_o + \frac{h}{2}J^{-1}\nabla H\left(\frac{y_1+y_o}{2}\right)\right) = y_o + hJ^{-1}\nabla H(y_o) + O(h^2) \]
\[ y(t_o+h) - y_1 = O(h^2) \]
por lo que resulta de orden mayor a 1.
Sin embargo, el esquema de MPR es \textit{simétrico} dado que se mantiene al hacer el cambio $y_{n+1}\to y_n$, $y_n\to y_{n+1}$ y $h\to -h$.
Como todo integrador simétrico debe tener orden par, entonces MPR resulta de orden 2 (o superior).

Además, podemos probar que este esquema es simpléctico
\[ \dpart{y_{n+1}}{y_n} = 1 + hJ^{-1}\dpart{}{y_n} \nabla H\left(\frac{y_{n+1}+y_n}{2}\right) = 1 + hJ^{-1} \nabla^2H\left(\frac{y_{n+1}+y_n}{2}\right)\frac{1}{2}\left[ 1+\dpart{y_{n+1}}{y_n} \right]\]
\[ \left[ 1 - \frac{h}{2}J^{-1} \nabla^2H\left(\frac{y_{n+1}+y_n}{2}\right) \right]\dpart{y_{n+1}}{y_n} =  1 + \frac{h}{2}J^{-1} \nabla^2H\left(\frac{y_{n+1}+y_n}{2}\right)\]
donde representamos la identidad de $\mathbb{R}^{2d}$ $\mathbb{I}=1$.
Definiendo $E\equiv\frac{h}{2}J^{-1} \nabla^2H\left(\frac{y_{n+1}+y_n}{2}\right)$ podemos despejar $\dpart{y_{n+1}}{y_n}$
\[ \dpart{y_{n+1}}{y_n} = (1-E)^{-1}(1+E) \]

Buscamos ver que $\left(\dpart{y_{n+1}}{y_n}\right)^TJ\left(\dpart{y_{n+1}}{y_n}\right)= J$, pero antes mostraremos una propiedad útil de $J$ y $E$
\[ E^T J = \frac{h}{2}\left( J^{-1} \nabla^2H \right)^T J = \frac{h}{2} \nabla^2H(J^{-1})^T  J = -\frac{h}{2} \nabla^2H = -JE\]
\[ (1 + E^T)^{-1}J = \sum_{n\geq0} \left(E^T\right)^n J = J\sum_{n\geq0} (-1)^nE^n = J(1 - E)^{-1}  \]

Ahora si podemos probar la simplecticidad de MPR
\[ \left(\dpart{y_{n+1}}{y_n}\right)^TJ\left(\dpart{y_{n+1}}{y_n}\right) = (1+E^T)(1-E^T)^{-1}J(1-E)^{-1}(1+E) = J(1-E)(1+E)^{-1}(1-E)^{-1}(1+E) = J\]

Sin embargo, es inmediato notar que el esquema de MPR no es explícito; no es posible obtener $y_{n+1}$ como una combinación de funciones conocidas aplicadas a $y_n$.
Otro método simpléctico ampliamente utilizado es \textit{Velocity-Verlet} que,a demás de tener el mismo orden resulta explícito para hamiltoneanos separables
\[ H(\mathbf{q}_1,..,\mathbf{q}_N,\mathbf{p}_1,..,\mathbf{p}_N) = \sum_i \frac{p_i^2}{2m} + U(\mathbf{q}_1,..,\mathbf{q}_N)\]

El esquema tiene dos versiones, pero la más utilizada (y que resulta explícita) es
\begin{align*}
 p_{n+1/2} &= p_n - \frac{h}{2}\dpart{H}{q}(p_{n+1/2}, q_n) \\
 q_{n+1} &= q_n + \frac{h}{2}\left( \dpart{H}{p}(p_{n+1/2}, q_n) + \dpart{H}{p}(p_{n+1/2}, q_{n+1}) \right) \\
 p_{n+1} &= p_{n+1/2} - \frac{h}{2}\dpart{H}{q}(p_{n+1/2}, q_{n+1})
\end{align*}
que para estos hamiltoneanos resulta explicitamente
\begin{align*}
 p_{n+1/2} &= p_n - \frac{h}{2}\dpart{U}{q}(q_n) \\
 q_{n+1} &= q_n + \frac{h}{m}p_{n+1/2} \\
 p_{n+1} &= p_{n+1/2} - \frac{h}{2}\dpart{U}{q}(q_{n+1}) = p_n - \frac{h}{2} \left( \dpart{U}{q}(q_n) + \dpart{U}{q}(q_{n+1}) \right)
\end{align*}

Sin embargo, para hamiltoneanos de la forma
\[ H(\mathbf{q}_1,..,\mathbf{q}_N,\mathbf{p}_1,..,\mathbf{p}_N) = \sum_i \frac{p_i^2}{2m} + U(\mathbf{q}_1,..,\mathbf{q}_N;\mathbf{p}_1,..,\mathbf{p}_N)\]
se vuelve nuevamente implícito.
Esta es una característica común a todos los integradores simplécticos: solo resultan explícitos para hamiltoneanos separables.

Esto resulta muy relevante, dado que implica que el uso de Dinámica Molecular para sistemas interactuantes mediante potencial de Pauli requerirá además la implementación
de algún método para resolver ecuaciones implícitas como \eqref{eq:MPR}.
El costo computacional de estas implementaciones resulta prohibitivo para sistemas de muchas partículas, como mostraremos más adelante.



\subsection{Método de Metropolis-Montecarlo}

En principio, conocer la función de partición $Z$ otorga acceso a todas las magnitudes termodinámicas de interés.
Para un sistema de $N$ partículas no interactuantes, el hamiltoniano total del sistema puede escribirse como la suma de hamiltonianos de una partícula
\[H(\mathbf{q}_1,..,\mathbf{q}_N;\mathbf{p}_1,..,\mathbf{p}_N) = H_1(\mathbf{q}_1, \mathbf{p}_1) + ... +H_N(\mathbf{q}_N, \mathbf{p}_N)\]
lo cual permite inmediatamente factorizar la función de partición canónica
\[ Z = \int e^{-\beta H(\mathbf{q}_1,..,\mathbf{q}_N;\mathbf{p}_1,..,\mathbf{p}_N)} d^{3N}qd^{3N}p = \prod_{i=1}^N \int e^{-\beta H_i(\mathbf{q};\mathbf{p})} d^{3}qd^{3}p = \prod_{i=1}^N Z_i \]

En general, habrá $n\ll N$ funciones de partición distintas, donde $n$ es la cantidad de tipos de partículas (equivalentemente, la cantidad de hamiltonianos diferentes) y el computo de $Z$ resulta simple.

Para sistemas interactuantes, este método no es aplicable al no ser el hamiltoniano separable; excepto en casos particulares, el cálculo de $Z$ para sistemas interactuantes resulta analíticamente imposible.
Aunque calcular los pesos $e^{-\beta H(\mathbf{q}_1,..,\mathbf{q}_N;\mathbf{p}_1,..,\mathbf{p}_N)}$ para una dada configuración es posible, el cómputo de $Z$ exige barrer el espacio de fases $6N$-dimensional,
cuyo volumen crece exponencialmente con $N$.

Para temperaturas finitas, sin embargo, gran parte de este volumen generará un aporte ínfimo a la función de partición $Z$, al ser estados cuyo peso relativo es bajo.
Por esto mismo, tendrán un aporte ínfimo en los valores medios de las magnitudes termodinámicas de interés.
Por lo tanto, buscamos un método que nos permita evitar por completo el cómputo de $Z$ y tome en cuenta solo las configuraciones relevantes para el sistema a una dada temperatura $T$.
El método de Metropolis-Montecarlo (también conocido como Metropolis-Hastings) nos permite lograr esto mediante el uso de cadenas de Markov.

\subsubsection{Cadenas de Markov}

Las cadenas de Markov son ampliamente utilizadas para modelar múltiples fenómenos (biologicos, economicos, sociales) por su simplicidad y teoría bien desarrollada.
El objetivo de las cadenas de Markov es introducir la memoria en una sucesión de variables aleatorias $(X_n)_{n\in \mathbb{N}}$ de la forma más simple posible.
Esto es, la evolución $X_n \to X_{n+1}$ no depende de los $n-1$ estados previos ni del 'tiempo' $n$, sino únicamente del estado actual $X_n$.

Con esto en mente, una sucesión de variables aleatorias $(X_n)_{n\in \mathbb{N}}$ es una cadena de Markov a tiempo discreto si
\[ P(X_{n+1}=j | X_n = i, X_{n-1} = i_{n-1}, ..., X_o = i_0) = P(X_{n+1}=j | X_n = i)  \]
donde $j, i, i_{n-1}, ..., i_0$ son estados posibles de la cadena.

Se define la matriz de transición $p(i,j)$ de la cadena de Markov como la probabilidad de alcanzar el estado $j$ partiendo del estado $i$, aprovechando la independencia del tiempo $n$
\[ p(i,j) = P(X_{n+1} = j | X_n = i) \]

En particular, si nos interesa ver las probabilidades de transición múltiples pasos adelante (a $n$ pasos)
\begin{align*}
 p_n(i,j) = P(X_n = j | X_0 = i) &= \frac{P(X_n = j, X_0 = i)}{P(X_0 = i)} \\
 &= \sum_k \frac{P(X_n = j, X_{n-1}=k, X_0 = i)}{P(X_0 = i)} \\
 &= \sum_k P(X_n = j| X_{n-1}=k, X_0 = i)\frac{P(X_{n-1}=k, X_0 = i)}{P(X_0 = i)} \\
 &= \sum_k P(X_n = j| X_{n-1}=k)P(X_{n-1}=k| X_0 = i) = \sum_k p(k,j)p_{n-1}(i,k)
\end{align*}

Identificando la suma anterior como el producto de matrices $(p_{n-1} \cdot p)(i,j)$ y usando que $p_1(i,j)=p(i,j)$, resulta inmediato que $p_n(i,j) = p^n(i,j)$.
En esta propiedad reside la simplicidad de las cadenas de Markov.

En particular, nos interesan las cadenas de Markov irreducibles; aquellas en las que todo estado $i$ puede alcanzar el estado $j$ en una cantidad finita de pasos.
Matemáticamente, decimos que una cadena de Markov es irreducible si
\[ \forall i,j \quad \exists n\in \mathbb{N} \text{ tal que } p^n(i,j) \neq 0 \]

El caso más simple de una cadena no irreducible es una con dos conjuntos de estados $A$ y $B$ desconectados entre si.
\[ p =
\begin{pmatrix}
 p_A & 0\\
 0 & p_B
\end{pmatrix} \]
En estos casos, resultaría mucho más sensato tratarla como dos cadenas de Markov de estados $A$ y $B$ con sus respectivas matrices $p_A$ y $p_B$.

El otro caso de cadena no irreducible es aquella con estados absorventes; aquellos que no pueden transicionar a otros estados.
Decimos que un estado $i$ es absorvente si $p(i,j)=\delta_{ij} \text{ } \forall j$.
Estas cadenas, en principio, pueden utilizarse para modelar ciertos problemas como la 'Ruina del jugador'. (¿NOTA AL PIE?)

En el método de Montecarlo, las cadenas de Markov utilizadas son irreducibles dado que tienen lo que se conoce como una \textit{distribución estacionaria}, que describimos a continuación.

\subsubsection{Distribución estacionaria}

Cuando el estado inicial de una cadena de Markov tiene una distribución $q(i)$, podemos usar el resultado anterior para calcular la distribución $n$ pasos después
\[ P(X_n=j) = \sum_i P(X_n=j,X_0=i) =  \sum_i P(X_n=j|X_0=i)P(X_0=i) = \sum_i p^n(i,j)q(i) = (q\cdot p^n)(j)\]
usando el producto habitual de matrices y tomando el vector fila de probabilidades $q$.
Por lo tanto, la distribución de probabilidad a tiempo $n$ se relaciona con la inicial segun $q_n = q_0\cdot p^n$ o, recursivamente, $q_{n+1} = q_n\cdot p$.

Resulta natural entonces preguntarse si existirá alguna forma de mantener la distribución constante $q_{n+1} = q_n$.
Esto es equivalente a pedir que $q_n$ sea un autovector de $p$ con autovalor 1.
Para diferenciar, llamaremos $\pi(x)$ a esta distribución estacionaria que cumple $\pi p = \pi$.

Obtener $\pi$ implica resolver las \textit{ecuaciones de balance}
\[ \sum_k \pi(k) p(k, j) = \pi(j) \]

Sin embargo, muchas veces resulta más natural el uso de las \textit{condiciones de balance detallado}
\[ \pi(k) p(k, j) = \pi(j)p(j, k) \text{ } \forall j,k \]
que nos arroja inmediatamente las ecuaciones de balance al sumar sobre $k$.
\[ \sum_k \pi(k) p(k, j) = \sum_k \pi(j)p(j, k) = \pi(j) \sum_k p(j,k) = \pi(j) \]

Claramente, la condición de balance detallado resulta más fuerte que la de balance y no necesariamente existe.
Sin embargo, cuando existe, puede ser mucho más sencilla de encontrar.
Esto es particularmente cierto para el caso de matrices de transición $p$ ralas (con gran cantidad de ceros), donde las condiciones de balance detallado se simplifican.

A continuación, enunciaremos dos teoremas muy relevantes para el método de Metropolis.
El primero asegura la existencia de una única distribución estacionaria como solución a las ecuaciones de balance (pero no necesariamente balance detallado).

\begin{theorem}
 Sea $p$ una matriz de transición finita e irreducible. Luego, existe solución única a las ecuaciones de balance $\pi p = \pi$ con $\sum_k \pi(k) = 1$ y $\pi(k)\geq 0$ $\forall k$
\end{theorem}

Es importante aclarar que la hipotesis de matriz finita es necesaria para poder asegurar la normalización de $\pi$.
Existe una versión del teorema para dimensión infinita, pero no es relevante para nuestro tratamiento actual, donde nos limitaremos a un conjunto de estados finito.
Volveremos sobre esto más adelante.

Pero la existencia de una distribución estacionaria puede no ser suficiente si somos incapaces de calcularla.
Si la cadena de Markov comienza con una distribución inicial $q(i)$ cualquiera, esperariamos que eventualmente alcance la distribución estacionaria.
El siguiente teorema es análogo a la Ley de los Grandes Números para el caso de cadenas de Markov y nos asegura lo anterior, entre otras cosas.

\begin{theorem}{\label{teo:markov_muestreo}}
 Sea $p$ una matriz de transición irreducible con distribución estacionaria $\pi$ y $f$ una función sobre los estados tal que $\sum_x f(x)\pi(x) < \infty$, entonces
 \[ \lim_{n\to\infty} \frac{1}{n} \sum_{m=1}^n f(X_m) = \sum_x f(x)\pi(x) \]
\end{theorem}

Para obtener la convergencia, basta tomar la función $f(x) = \delta_{xy}$ para algún $y$, de forma tal que $\sum_{m=1}^n f(X_m)$ sea la cantidad de tiempo que el sistema estuvo en $y$.
Por el teorema anterior, esto converge a $\pi(y)$.
Pero este teorema es más general y nos permite obtener distintos observables $f$ de la distribución, propiedad que nos resultará muy útil cuando expliquemos el algoritmo.


\subsubsection{Algoritmo de Metropolis-Montecarlo}{\label{sec:alg_mm}}

El objetivo de este algoritmo es obtener muestras de una dada distribución $\pi$ construyendo una cadena de Markov que la tenga como distribución estacionaria.
Para esto, fabricaremos una matriz de transición $p$ que cumpla las condiciones de balance detallado con $\pi$.

Comenzamos con una cadena de Markov con matriz $q(x,y)$ representando la transición propuesta.
Sin embargo, aceptaremos la transición con probabilidad
\[r(x,y) = \min \left\{ \frac{\pi(y)q(y,x)}{\pi(x)q(x,y)}, 1\right\}\]
de forma tal que la probabilidad de transición final resulta $p(x,y) = q(x,y)r(x,y)$.

Veamos que esta $p$ cumple las ecuaciones de balance detallado.
Supongamos $\pi(y)q(y,x) \leq \pi(x)q(x,y)$ tal que $r(x,y) = \pi(y)q(y,x)/\pi(x)q(x,y)$ y $r(y,x) = 1$.
El caso opuesto resulta completamente análogo.
\[ \pi(x) p(x, y) = \pi(x) q(x,y)\frac{\pi(y)q(y,x)}{\pi(x)q(x,y)} = \pi(y)q(y,x) = \pi(y)q(y,x)r(y,x) = \pi(y)p(y,x) \]

Para generar muestras con distribución $p$, basta evolucionar temporalmente la cadena durante un tiempo suficientemente largo hasta que alcance el equilibrio y
aprovechamos el teorema \ref{teo:markov_muestreo} para muestrear los valores medios de las magnitudes termodinámicas de interés.

En nuestro caso, tomamos los estados $x = (\mathbf{q}_1,..,\mathbf{q}_N;\mathbf{p}_1,..,\mathbf{p}_N)$ como distintas configuraciones del espacio de fases.
Queremos como distribución estacionaria la predicha por el ensamble canónico \[\pi(x) = \frac{e^{-\beta E_x}}{Z} = \frac{1}{Z}e^{-\beta H(\mathbf{q}_1,..,\mathbf{q}_N;\mathbf{p}_1,..,\mathbf{p}_N)}\]
Como dijimos, este algoritmo nos ahorra el problema de calcular la función de partición $Z$, dado que las probabilidades de transición $p$ dependen del cociente de los $\pi(x)$.

Sin embargo, el espacio de fases $6N$-dimensional es continuo y, por lo tanto, la cantidad de estados $x$ es infinita.
Esto no es problemático dado toda simulación computacional fuerza el problema a ser finito.
Por lo tanto, el espacio de fases pasa a ser una red finita $6N$-dimensional y podemos usar todo el aparato anterior.

Podemos elegir esta matriz de transición proponiendo pasos en los que seleccionamos una partícula $i$ con probabilidad $1/N$, moviendo sus coordenadas canónicas a otras dentro de un paralelepipedo
centrado en su valor actual
\[
\left\{\begin{matrix}
\mathbf{q}_i \to  \mathbf{q}_i + \Delta_q \mathbf{a} & & a_k,b_k \text{ variables aleatorias independientes y }  \\
\mathbf{p}_i \to  \mathbf{p}_i + \Delta_p \mathbf{b} & &  \text{uniformes en } \{2\frac{j}{M}-1: j\in\mathbb{Z}, 0 \leq j \leq M\}
\end{matrix}\right.\]
donde $M$ suele ser el máximo entero representable por la computadora $M = 2^{31}-1\sim 10^9$.

Por lo tanto, si $x$ e $y$ difieren en las coordenadas de más de una partícula tenemos $q(x,y)=0$.
Si difieren unicamente en las coordenadas de la partícula $i$, tenemos
\[q(x,y) = \frac{\Theta(\Delta_q - \Arrowvert \mathbf{q}_i - \mathbf{q}_i' \Arrowvert_\infty)\Theta(\Delta_p - \Arrowvert \mathbf{p}_i - \mathbf{p}_i' \Arrowvert_\infty)}{N(2M+1)^6}\]
por lo que nuestra elección de $q$ es simétrica con $q(x,y) = q(y,x)$.

Aprovechando la simetría de $q$, la probabilidad de aceptación resulta se simplifica a
\[r(x,y) = \min\left\{\frac{\pi(y)}{\pi(x)}, 1\right\} = \min\{e^{-\beta(E_y-E_x)}, 1\}\]
De esta forma, si tenemos que $\Delta E = E_y - E_x \leq 0$ aceptamos inmediatamente. Si resulta $\Delta E > 0$, aceptamos con probabilidad $e^{-\beta\Delta E}$.


\subsection{Materia Nuclear}{\label{sec:intro_NM}}

\textbf{CITAR TESIS MOLINELLI}
La Materia Nuclear (NM) consiste en un sistema termodinámico de nucleones electricamente neutros, interactuando unicamente mediante un potencial nuclear efectivo (sin Coulomb).
En general, cuantificamos mediante $\delta=(N_n - N_p)/N$ la asimetría entre la cantidad neutrones $N_n$ y protones $N_p$.
Para $\delta=0$, decimos que el sistema es \textit{simétrico}.
Este parámetro se suma a los habituales $T$ y $\rho$ a la hora de obtener una ecuación de estado $U(T,\rho, \delta)$.

Ciertamente, no debe confundirse NM con la materia nuclear de los átomos.
La formulación de NM asume sistemas infinitos, extensivos y neutros muy diferentes a los nucleos atómicos.
Esta cuestión dificulta la vinculación directa de los resultados teóricos con los obtenidos experimentalmente.
